Adaboost Classification Project - Problem Statement

1.Problem statement
.this dataset comprises used cars sold on cardekho.com in india as well as important features of these cars.
.if user can predict the price of the car based on input features.
.Prediction results can be used to give new seller the price suggestion based on market condition.
2.Data Collection.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# import plotly.express as px
import warnings

warnings.filterwarnings('ignore')

%matplotlib inline

# load data
df = pd.read_csv(r"./dataset/cardekho_dataset.csv", index_col=[0])

df.head()

3.Data Cleaning
Handling Missing values , dublicates, check data type, understand dataset.
#check null value
# check features with nan value
df.isnull().sum()
# Remove Unneccessary columns
df.drop(columns=['car_name', 'brand'], inplace=True, errors='ignore')

df.head()
df['model'].unique()
# Getting All Different Types of features
num_features = [feature for feature in df.columns if df[feature].dtype != 'o']
print("Num of Numerical Features :",len(num_features))

#categorial features
cat_features = [feature for feature in df.columns if df[feature].dtype == 'o']
print("Num of Categorical features :", len(cat_features))

# Discreate features
discrete_features = [feature for feature in num_features if len(df[feature].unique())<=25]
print("Num of Discrete Features :", len(discrete_features))

#continuous Features
continueus_feature = [feature for feature in num_features  if feature not in discrete_features]
print("Num of Continuous Features :", len(continueus_feature))

# create price categories
df['price_class'] = pd.cut(
    df['selling_price'],
    bins=[0, 300000, 700000, df['selling_price'].max()],
    labels=[0, 1, 2]   # 0=Low, 1=Medium, 2=High
)

df['price_class'].value_counts()

x = df.drop(['selling_price', 'price_class'], axis=1)
y = df['price_class']

Feature Encoding and Scalling
one Hot Encoding for columns which had lesser unique values adn ordinal

.one hot encoding is a process by which categorical variables are converted into a from  that could be provided to ML algorithms to do a better job in predicton.
#len of model
len(df['model'].unique())
#value count
df['model'].value_counts()
#label encoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
x['model'] = le.fit_transform(x['model'])

x.head()

#seller_type
len(df['seller_type'].unique()),len(df['fuel_type'].unique()),len(df['transmission_type'].unique())
num_features = x.select_dtypes(include=['int64', 'float64']).columns
cat_features = x.select_dtypes(include=['object']).columns

onehot_columns = ['seller_type', 'fuel_type', 'transmission_type']

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

numeric_transformer = StandardScaler()
oh_transformer = OneHotEncoder(drop='first', sparse_output=False)

preprocessor = ColumnTransformer(
    transformers=[
        ('OneHot', oh_transformer, onehot_columns),
        ('Scaler', numeric_transformer, num_features)
    ],
    remainder='drop'
)

x_transformed = preprocessor.fit_transform(x)




# separate dataset into train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_transformed, y, test_size=0.2 , random_state=42)
x_train.shape, x_test.shape


MODEL Training And Model Selection
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import  accuracy_score, f1_score, recall_score, roc_auc_score,precision_score
                           
## Beginning Model Training
models = {
   
    "Random Forest": RandomForestClassifier(),
    "AdaBoost": AdaBoostClassifier()
}


for name, model in models.items():

    
    model.fit(x_train, y_train)


    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)

    #Training set performance
    model_train_accuracy = accuracy_score(y_train, y_train_pred) # calculate accuracy
    model_train_f1 = f1_score(y_train, y_train_pred, average='weighted')
    model_train_precison = precision_score(y_train, y_train_pred, average='weighted')
    model_train_recall = recall_score(y_train, y_train_pred, average='weighted')
    # model_train_roc = roc_auc_score(y_train, y_train_proba, multi_class='ovr', average='weighted')

    # Test set performance
    model_test_accuracy = accuracy_score(y_test, y_test_pred) # calculate accuracy
    model_test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    model_test_precison = precision_score(y_test, y_test_pred, average='weighted')
    model_test_recall = recall_score(y_test, y_test_pred , average='weighted')
    # model_test_roc = roc_auc_score(y_test, y_test_proba, multi_class='ovr', average='weighted')

    print(f"{name} trained successfully")

    # print("")
    print("---------Model--Train--performace--------------------")
    print("accuracy_score :{:.4f}".format(model_train_accuracy))
    print("F1_score :{:.4f}".format(model_train_f1))
    # print("Precision :{:.4f}".format(model_train_precison))
    # print("Recall :{:.4f}".format(model_train_recall))

    print("------Model--performance--for--Test---")

    print("accuracy_score :{:.4f}".format(model_test_accuracy))
    print("F1_score :{:.4f}".format(model_test_f1))
    # print("Precision :{:.4f}".format(model_test_precison))
    # print("Recall :{:.4f}".format(model_test_recall))

    print("="*35)
    print("\n")
    
## Initialize few parameter for Hyperparamter tuning
# knn_parms = {"n_neighbors":[2, 3, 10, 20, 40, 50]}
rf_params = {"max_depth":[5, 8, 15, None, 10],
             "max_features":[5, 7, "auto", 8],
             "min_samples_split":[2, 8, 15, 20],
             "n_estimators":[100 , 200, 500, 1000]
             }
adaboost_pram = {
    "n_estimators":[50, 60, 70, 80, 90],
    "algorithm":['SAMME','SAMME.R']
}

rf_params
# MOdels list for Hyperparameter tuning
randomcv_models = [
                   ('Ab', AdaBoostClassifier(), adaboost_pram),
                   ("RF",RandomForestClassifier(), rf_params)
                   ]



# Hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV

model_param ={}
for name, model, params in randomcv_models:
    random = RandomizedSearchCV(estimator=model,
                                param_distributions=params,
                                n_iter=5,
                                cv=2,
                                verbose=2,
                                n_jobs=-1,
                                random_state=42)
    random.fit(x_train, y_train)
    model_param[name] = random.best_params_

# print(model_param)
for model_name in model_param:
    print(f"------------------Best Params for {model_name}----------------------")
    print(model_param[model_name])
## Retraining the models with best parameters

models = {
    "Random Forest Classificaion": RandomForestClassifier(
        n_estimators=1000,
        max_depth=5,
        min_samples_split=20,
        max_features=8, 
        min_samples_leaf=1,
        random_state=42,
        n_jobs=-1
    ),
    
    "AdaboostClassifier": AdaBoostClassifier(
        n_estimators=60,
        algorithm="SAMME"
    )
}

for name, model in models.items():

    model.fit(x_train, y_train)

    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)

    y_train_proba = model.predict_proba(x_train)
    y_test_proba = model.predict_proba(x_test)
    #Training set performance
    model_train_accuracy = accuracy_score(y_train, y_train_pred) # calculate accuracy
    model_train_f1 = f1_score(y_train, y_train_pred, average='weighted')
    model_train_precison = precision_score(y_train, y_train_pred, average='weighted')
    model_train_recall = recall_score(y_train, y_train_pred, average='weighted')
    model_train_roc = roc_auc_score(y_train, y_train_proba, multi_class='ovr', average='weighted')

    # Test set performance
    model_test_accuracy = accuracy_score(y_test, y_test_pred) # calculate accuracy
    model_test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    model_test_precison = precision_score(y_test, y_test_pred, average='weighted')
    model_test_recall = recall_score(y_test, y_test_pred , average='weighted')
    model_test_roc = roc_auc_score(y_test, y_test_proba, multi_class='ovr', average='weighted')

    print(name)
    print("---------Model--Train--performace--------------------")
    print("accuracy_score :{:.4f}".format(model_train_accuracy))
    print("F1_score :{:.4f}".format(model_train_f1))
    # print("Precision :{:.4f}".format(model_train_precison))
    # print("Recall :{:.4f}".format(model_train_recall))
    print("roc_auc_score :{:.4f}".format(model_train_roc))

    print("------Model--performance--for--Test---")

    print("accuracy_score :{:.4f}".format(model_test_accuracy))
    print("F1_score :{:.4f}".format(model_test_f1))
    # print("Precision :{:.4f}".format(model_test_precison))
    # print("Recall :{:.4f}".format(model_test_recall))
    print("roc_auc_score :{:.4f}".format(model_test_roc))

    print("=" * 40, "\n")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, roc_auc_score

classes = np.unique(y_test)
y_test_bin = label_binarize(y_test, classes=classes)

y_test_proba = model.predict_proba(x_test)

plt.figure(figsize=(7,6))

for i, cls in enumerate(classes):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_test_proba[:, i])
    auc_score = roc_auc_score(y_test_bin[:, i], y_test_proba[:, i])

    plt.plot(fpr, tpr, label=f"Class {cls} (AUC = {auc_score:.2f})")

plt.plot([0,1], [0,1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multiclass ROC Curve (OvR)')
plt.legend()
plt.grid(True)
plt.show()
